{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_trading_env\n",
    "from gym_trading_env.downloader import download, EXCHANGE_LIMIT_RATES\n",
    "#import gymnasium as gym # No longer used and is installed with other libraries anyways\n",
    "import gymnasium as gym\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # used to get current directory\n",
    "from tqdm import tqdm\n",
    "# Import all the Pearl related methods\n",
    "from pearl.pearl_agent import PearlAgent\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "from pearl.policy_learners.sequential_decision_making.deep_q_learning import (\n",
    "    DeepQLearning,\n",
    ")\n",
    "from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import (\n",
    "    FIFOOffPolicyReplayBuffer,\n",
    ")\n",
    "from pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from pearl.policy_learners.sequential_decision_making.implicit_q_learning import ImplicitQLearning\n",
    "from pearl.utils.functional_utils.train_and_eval.offline_learning_and_evaluation import (\n",
    "    get_offline_data_in_buffer,\n",
    "    offline_evaluation,\n",
    "    offline_learning,\n",
    ")\n",
    "from pearl.neural_networks.sequential_decision_making.actor_networks import VanillaContinuousActorNetwork\n",
    "from pearl.pearl_agent import PearlAgent\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "from pearl.policy_learners.sequential_decision_making.deep_q_learning import (\n",
    "    DeepQLearning,\n",
    ")\n",
    "from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import (\n",
    "    FIFOOffPolicyReplayBuffer,\n",
    ")\n",
    "from pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "import torch # used for calculating the mean when evaluating Pearl training performance\n",
    "\n",
    "##FOR USE WITH VANILLAQVALUENETWORK\n",
    "#from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork\n",
    "\n",
    "experiment_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reward funtions\n",
    "\n",
    "# Define reward function here. Current function is based on last 2 timesteps (one hour apart)\n",
    "def log_reward_function(history):\n",
    "        return np.log(history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2])\n",
    "\n",
    "def diff_reward_function(history):\n",
    "        return history[\"portfolio_valuation\", -1] - history[\"portfolio_valuation\", -2]\n",
    "def cumulative_reward_function(history):\n",
    "        return np.diff(history[\"portfolio_valuation\", -1] , history[\"portfolio_valuation\", 0])/len(history[\"portfolio_valuation\"])\n",
    "\n",
    "def shape_reward_function(history):\n",
    "        return history[\"portfolio_valuation\", -1] /np.std(history[\"portfolio_valuation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [\n",
    "    [ -1, 0, 1],\n",
    "    [-.5,0,.5],\n",
    "    [-1,-.5,0,.5,1],\n",
    "    [-1,1],\n",
    "    ]\n",
    "reward_functions=[shape_reward_function,\n",
    "                  log_reward_function,\n",
    "                  diff_reward_function,\n",
    "                  cumulative_reward_function,\n",
    "                  ]\n",
    "hidden_dims=[[64, 64],[128,128],[256,256]]\n",
    "\n",
    "num_layers=[2,3,4]\n",
    "num_dims=64\n",
    "training_rounds=20\n",
    "\n",
    "def make_arch(num_layers,num_dims):\n",
    "    return[num_dims for n in range(num_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 64]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_arch(num_layers=num_layers,num_dims=num_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1e-05'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={\n",
    "    'num_layers':hp.choice('num_layers',num_layers),\n",
    "    'reward_function':hp.choice(reward_functions),\n",
    "    'learning_rate':hp.loguniform('learning_rate',1e-5,1e-2)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = train_env.action_space.n\n",
    "agent = PearlAgent(\n",
    "    policy_learner=DeepQLearning(\n",
    "        state_dim=train_env.observation_space.shape[0],\n",
    "        action_space=train_env.action_space, \n",
    "        hidden_dims=[64, 64],\n",
    "        \n",
    "        training_rounds=20,\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=num_actions\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(10_000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(positon,reward_function):\n",
    "    test_env = gym.make(\"TradingEnv\",\n",
    "        name= \"ICPUSD\",\n",
    "        df = test_df,\n",
    "        positions = [ -1, 0, 1], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        reward_function = reward_function,\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_train_agent(gamma,lr,hidden_dems,num_episodes,train_after_episode):\n",
    "    agent=make_agent(gamma,lr,hidden_dems)\n",
    "    \n",
    "\n",
    "    test_env\n",
    "\n",
    "    return profit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wairl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
